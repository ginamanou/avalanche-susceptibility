{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e2bfda",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook was created while conducting data cleaning and manipulation of historical, meteorological data from weather stations all over Andorra.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a4248d",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea29bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (15, 7), 'figure.dpi': 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd1b39",
   "metadata": {},
   "source": [
    "## Block 1\n",
    "To make the unit transformations for temp_max, temp_min, rain and save to the local folder 3_units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/2_end_date\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # Get the text before .csv into a string variable named \"name\"\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "\n",
    "#     df = pd.read_csv(f)\n",
    "    \n",
    "#     # Change the units where necessary (check metadata_neu for more info)\n",
    "#     if name != 'Ransol':\n",
    "#         df[\"temp_min\"] = df['temp_min'].div(10)\n",
    "#         df[\"temp_max\"] = df['temp_max'].div(10)\n",
    "#         df[\"rain\"] = df['rain'].div(10)\n",
    "    \n",
    "#     # Save each station with the necessary unit transformations to a new folder named \"units\"\n",
    "#     path = \"D:/Allaus/Data_analysis/Neu_from_meteo/3_units\"\n",
    "#     df.to_csv(Path(path, name + '.csv'), index=False)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f5f523",
   "metadata": {},
   "source": [
    "## Block 2\n",
    "To keep only the months December to April for the winter stations.\n",
    "\n",
    "To remove incorrect values from wind_dir and snowdrift.\n",
    "\n",
    "To save to local folder 4_wind_dir_metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1526a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/3_units\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # To grab just the name of each file\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "#     print(name)     # for debugging\n",
    "    \n",
    "#     # To read the data\n",
    "#     if name != 'Ransol':\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'] + df['hour'], format='%m/%d/%Y%H:%M')    # alternative 1\n",
    "#         #df['date'] = pd.to_datetime(df['date'] + ' ' + df['hour'])                      # alternative 2\n",
    "#         df.drop(columns=['hour_drop', 'hour', 'id'], inplace=True)\n",
    "#         #df.set_index('date', inplace=True)\n",
    "#     else:\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')\n",
    "#         df.drop(columns=['id'], inplace=True)\n",
    "#         #df.set_index('date', inplace=True)\n",
    "\n",
    "#     #print(df)       # for debugging\n",
    "#     #print(df.dtypes)\n",
    "    \n",
    "#     # To keep only the months December to April for the winter stations\n",
    "#     df = df[~df[\"date\"].dt.month.isin([5,6,7,8,9,10,11])]\n",
    "#     df = df.reset_index()\n",
    "#     df = df.drop(columns=['index'])\n",
    "#     #print(df)\n",
    "    \n",
    "#     # To remove from the wind_dir the values that are outide of the metadata list. And put NaN instead.\n",
    "#     # In combination with the next chunk of code to see if it worked.\n",
    "#     directions = [0, 4, 9, 13, 18, 22, 27, 31, 36, 99]\n",
    "#     if name != 'Ransol':   # I only found data out of the metadata for wind_dir, not for snowdrift\n",
    "#         for i in range(len(df)-1):\n",
    "#             if (df.loc[i, 'wind_dir'] not in directions) & (df.loc[i, 'wind_dir'] == df.loc[i, 'wind_dir']):\n",
    "#                 df.loc[i, 'wind_dir'] = float('nan')\n",
    "    \n",
    "#     # To check that the values of snowdrift and wind direction belong to the metadata or are NaN (see above)\n",
    "#     directions = [0, 4, 9, 13, 18, 22, 27, 31, 36, 99]\n",
    "#     transport = [0,1,2,3,4,5,6,7,8,9, '/']\n",
    "#     if name != 'Ransol':\n",
    "#         count_dir = 0\n",
    "#         count_trans = 0\n",
    "#         for i in range(len(df)-1):\n",
    "#             if (df.loc[i, 'wind_dir'] not in directions) & (df.loc[i, 'wind_dir'] == df.loc[i, 'wind_dir']):\n",
    "#                 #print(df.loc[i, 'wind_dir'])\n",
    "#                 count_dir = count_dir + 1\n",
    "#             if (df.loc[i, 'snowdrift'] not in transport) & (df.loc[i, 'snowdrift'] == df.loc[i, 'snowdrift']):\n",
    "#                 #print(df.loc[i, 'snowdrift'])\n",
    "#                 count_trans = count_trans + 1\n",
    "\n",
    "#     print(count_dir)\n",
    "#     print(count_trans)\n",
    "    \n",
    "#     # To save to the 4_wind_dir_metadata folder\n",
    "#     path = \"D:/Allaus/Data_analysis/Neu_from_meteo/4_wind_dir_metadata\"\n",
    "#     df.to_csv(Path(path, name + '.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d791c692",
   "metadata": {},
   "source": [
    "## Block 3\n",
    "To check that the snow_tot, snow_new, wind_dir, wind_vel, rain, snowdrift don't have negative values and save the corrected dataframes to csv files in the local folder 5_positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d8e381",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/4_wind_dir_metadata\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # To grab just the name of each file\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "#     print(name)     # for debugging\n",
    "    \n",
    "#     # To read the data\n",
    "#     df = pd.read_csv(f)\n",
    "#     df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "#     #df.set_index('date', inplace=True)\n",
    "#     #print(df)\n",
    "\n",
    "#     # I found that only Soldeu wind_vel had negative values\n",
    "#     if name == 'Soldeu':\n",
    "#         for i in range(len(df)-1):\n",
    "#             if df.loc[i, 'wind_vel'] < 0:\n",
    "#                 df.loc[i, 'wind_vel'] = float('nan')\n",
    "                \n",
    "#     # Check that there are no negative values of the respective variables for any of the stations            \n",
    "#     rain = 0\n",
    "#     snow_tot = 0\n",
    "#     snow_new = 0\n",
    "#     wind_vel = 0\n",
    "#     snowdrift = 0\n",
    "#     wind_dir = 0\n",
    "#     for i in range(len(df)-1):\n",
    "#         if df.loc[i, 'rain'] < 0:\n",
    "#             rain = rain + 1\n",
    "#         if df.loc[i, 'snow_tot'] < 0:\n",
    "#             snow_tot = snow_tot + 1\n",
    "#         if name != 'Ransol':\n",
    "#             if df.loc[i, 'snow_new'] < 0:\n",
    "#                 snow_new = snow_new + 1\n",
    "#             if df.loc[i, 'wind_vel'] < 0:\n",
    "#                 wind_vel = wind_vel + 1\n",
    "#             if df.loc[i, 'snowdrift'] < 0:\n",
    "#                 snowdrift = snowdrift + 1\n",
    "#             if df.loc[i, 'wind_dir'] < 0:\n",
    "#                 wind_dir = wind_dir + 1\n",
    "        \n",
    "#     print('rain:', rain)\n",
    "#     print('snow_tot:', snow_tot)\n",
    "#     print('snow_new:', snow_new)\n",
    "#     print('wind_vel:', wind_vel)\n",
    "#     print('snowdrift:', snowdrift)\n",
    "#     print('wind_dir:', wind_dir)\n",
    "\n",
    "#     #print(name)\n",
    "#     #print(df)\n",
    "\n",
    "# #     # To save to the 5_positive folder\n",
    "# #     path = \"D:/Allaus/Data_analysis/Neu_from_meteo/5_positive\"\n",
    "# #     df.to_csv(Path(path, name + '.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978bf50",
   "metadata": {},
   "source": [
    "## Block 4\n",
    "To correct wind_vel and wind_dir values (check metadata) and save to the local folder 6_dir_vel_correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435e6b9d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/5_positive\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # To grab just the name of each file\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "#     print(name)     # for debugging\n",
    "\n",
    "#     # To read the data\n",
    "#     if name != 'Ransol':\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y %H:%M')    \n",
    "#     else:\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')\n",
    "\n",
    "#     #print(df)\n",
    "    \n",
    "#     # 1st \n",
    "#     if name != 'Ransol':\n",
    "#         for i in range(len(df)-1):\n",
    "#             if (df.loc[i, 'wind_dir'] != df.loc[i, 'wind_dir']) & (df.loc[i, 'wind_vel'] == df.loc[i, 'wind_vel']):\n",
    "#                 df.loc[i, 'wind_vel'] = float('nan')\n",
    "\n",
    "#     count = 0\n",
    "#     x = list()\n",
    "#     if name != 'Ransol':\n",
    "#         for i in range(len(df)-1):\n",
    "#             if (df.loc[i, 'wind_dir'] != df.loc[i, 'wind_dir']) & (df.loc[i, 'wind_vel'] == df.loc[i, 'wind_vel']):\n",
    "#                 count = count + 1\n",
    "#                 x.append(i)\n",
    "\n",
    "#     print('1st:', count)\n",
    "#     #print(x)\n",
    "    \n",
    "#     # 2nd \n",
    "#     if name != 'Ransol':\n",
    "#         for i in range(len(df)-1):\n",
    "#             if (df.loc[i, 'wind_dir'] == df.loc[i, 'wind_dir']) & (df.loc[i, 'wind_vel'] != df.loc[i, 'wind_vel']):\n",
    "#                 df.loc[i, 'wind_dir'] = float('nan')\n",
    "    \n",
    "#     count = 0\n",
    "#     x = list()\n",
    "#     if name != 'Ransol':\n",
    "#         for i in range(len(df)-1):\n",
    "#             if (df.loc[i, 'wind_dir'] == df.loc[i, 'wind_dir']) & (df.loc[i, 'wind_vel'] != df.loc[i, 'wind_vel']):\n",
    "#                 count = count + 1\n",
    "#                 x.append(i)\n",
    "\n",
    "#     print('2nd:', count)\n",
    "#     #print(x)\n",
    "\n",
    "#     # 3rd\n",
    "#     if name != 'Ransol':\n",
    "#         for i in range(len(df)-1):\n",
    "#             if (df.loc[i, 'wind_dir'] == 0) & (df.loc[i, 'wind_vel'] != 0):\n",
    "#                 df.loc[i, 'wind_vel'] = float('nan')\n",
    "#                 df.loc[i, 'wind_dir'] = float('nan')\n",
    "                \n",
    "#     count = 0\n",
    "#     x = list()\n",
    "#     if name != 'Ransol':\n",
    "#         for i in range(len(df)-1):\n",
    "#             if (df.loc[i, 'wind_dir'] == 0) & (df.loc[i, 'wind_vel'] != 0):\n",
    "#                 count = count + 1\n",
    "#                 x.append(i)\n",
    "\n",
    "#     print('3rd:', count)\n",
    "#     #print(x)\n",
    "    \n",
    "#     # 4th\n",
    "#     if name != 'Ransol':\n",
    "#         for i in range(len(df)-1):\n",
    "#             if (df.loc[i, 'wind_dir'] != 0) & (df.loc[i, 'wind_vel'] == 0):\n",
    "#                 df.loc[i, 'wind_dir'] = float('nan')\n",
    "#                 df.loc[i, 'wind_vel'] = float('nan')\n",
    "\n",
    "#     count = 0\n",
    "#     x = list()\n",
    "#     if name != 'Ransol':\n",
    "#         for i in range(len(df)-1):\n",
    "#             if (df.loc[i, 'wind_dir'] != 0) & (df.loc[i, 'wind_vel'] == 0):\n",
    "#                 count = count + 1\n",
    "#                 x.append(i)\n",
    "\n",
    "#     print('4th:', count)\n",
    "#     #print(x)\n",
    "\n",
    "#     # To save to the 6_dir_vel_correct folder\n",
    "#     path = \"D:/Allaus/Data_analysis/Neu_from_meteo/6_dir_vel_correct\"\n",
    "#     df.to_csv(Path(path, name + '.csv'), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3538d81e",
   "metadata": {},
   "source": [
    "## Block 5\n",
    "To map the qualitative values of wind_dir and snowdrift and save to the local folder 7_map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7f3aa5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/6_dir_vel_correct\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # To grab just the name of each file\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "#     print(name)     # for debugging\n",
    "\n",
    "#     # To read the data\n",
    "#     if name != 'Ransol':\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y %H:%M')    \n",
    "#     else:\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')\n",
    "\n",
    "#     #print(df)\n",
    "\n",
    "#     # To map the qualitative values to wind_dir and snowdrift\n",
    "#     dict_dd_qual = {0:'calm wind', 4:'NE', 9:'E', 13:'SE', 18:'S', 22:'SW', 27: 'W', 31:'NW', 36:'N', 99:'variable wind'}\n",
    "# #     dict_dd_deg = {4: random.randint(23,67), 9: random.randint(68,112), 13: random.randint(113,157), \n",
    "# #                    18: random.randint(158,202), 22: random.randint(203,247), 27: random.randint(248,292), \n",
    "# #                    31: random.randint(293,337), 36: random.randint(*random.choice([(338, 360), (0, 22)]))}\n",
    "#     #dict_dd_deg = {4: 45, 9: 90, 13: 135, 18: 180, 22: 225, 27: 270, 31: 315, 36: 360}\n",
    "#     dict_cn_intensity = {0:'no transport', 1:'transport', 2:'moderate', 3:'moderate', 4:'moderate', 5:'moderate', \n",
    "#                          6:'strong', 7:'strong', 8:'strong', 9:'strong'}\n",
    "#     dict_cn_from = {2:'E', 3:'S', 4:'W', 5:'N', 6:'E', 7:'S', 8:'W', 9:'N'}\n",
    "    \n",
    "#     if name != 'Ransol':\n",
    "#         df['wind_dir_qual'] = df['wind_dir'].map(dict_dd_qual)\n",
    "#         #df['wind_dir_deg'] = df['wind_dir'].map(dict_dd_deg)\n",
    "#         df['snowdrift_intensity'] = df['snowdrift'].map(dict_cn_intensity)\n",
    "#         df['snowdrift_from'] = df['snowdrift'].map(dict_cn_from)\n",
    "    \n",
    "#     if name != 'Ransol':\n",
    "#         for i in range(len(df)-1):\n",
    "#             if df.loc[i, 'wind_dir'] == 4:\n",
    "#                 df.loc[i, 'wind_dir_deg'] = random.randint(23,67)\n",
    "#             elif df.loc[i, 'wind_dir'] == 9:\n",
    "#                 df.loc[i, 'wind_dir_deg'] = random.randint(68,112)\n",
    "#             elif df.loc[i, 'wind_dir'] == 13:\n",
    "#                 df.loc[i, 'wind_dir_deg'] = random.randint(113,157)\n",
    "#             elif df.loc[i, 'wind_dir'] == 18:\n",
    "#                 df.loc[i, 'wind_dir_deg'] = random.randint(158,202)\n",
    "#             elif df.loc[i, 'wind_dir'] == 22:\n",
    "#                 df.loc[i, 'wind_dir_deg'] = random.randint(203,247)\n",
    "#             elif df.loc[i, 'wind_dir'] == 27:\n",
    "#                 df.loc[i, 'wind_dir_deg'] = random.randint(248,292)\n",
    "#             elif df.loc[i, 'wind_dir'] == 31:\n",
    "#                 df.loc[i, 'wind_dir_deg'] = random.randint(293,337)\n",
    "#             elif df.loc[i, 'wind_dir'] == 36:\n",
    "#                 df.loc[i, 'wind_dir_deg'] = random.randint(*random.choice([(338, 360), (0, 22)]))\n",
    "#             else:\n",
    "#                 df.loc[i, 'wind_dir_deg'] = float('nan')\n",
    "  \n",
    "#     print(df)  #to check\n",
    "        \n",
    "#     # To save to the 7_map folder\n",
    "#     path = \"D:/Allaus/Data_analysis/Neu_from_meteo/7_map\"\n",
    "#     df.to_csv(Path(path, name + '.csv'), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946b029",
   "metadata": {},
   "source": [
    "## Block 6\n",
    "To remove outliers for temp_max and temp_min for all winter stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd59b5e2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/7_map\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # To grab just the name of each file\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "#     print(name)     \n",
    "\n",
    "#     # To read the data\n",
    "#     if name != 'Ransol':\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y %H:%M')    \n",
    "#                                                                            \n",
    "#                                                                            \n",
    "#     else:\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')\n",
    "        \n",
    "#     #print(df)\n",
    "\n",
    "#     print('before:')\n",
    "#     print(df.describe())\n",
    "    \n",
    "#     ## Z-score method\n",
    "    \n",
    "#     # For temp_max\n",
    "#     print('temp_max')\n",
    "    \n",
    "#     #sns.displot(df.temp_max)\n",
    "\n",
    "#     # find limits\n",
    "#     upper = df.temp_max.mean() + 3*df.temp_max.std()\n",
    "#     lower = df.temp_max.mean() - 3*df.temp_max.std()\n",
    "#     #print('upper limit:', upper)\n",
    "#     #print('lower limit:', lower)\n",
    "    \n",
    "#     # find outliers\n",
    "#     df.loc[(df.temp_max > upper) | (df.temp_max < lower)]   # to check\n",
    "#     print(len(df.loc[(df.temp_max > upper) | (df.temp_max < lower)]))  # to check\n",
    "    \n",
    "#     # trimming - replace the outlier data with NaN values\n",
    "#     for i in range(len(df)-1):\n",
    "#         if (df.loc[i, 'temp_max'] > upper) | (df.loc[i, 'temp_max'] < lower):\n",
    "#             df.loc[i, 'temp_max'] = float('nan')\n",
    "    \n",
    "#     #sns.displot(df.temp_max)\n",
    "    \n",
    "#     # For temp_min\n",
    "#     print('temp_min')\n",
    "    \n",
    "#     #sns.displot(df.temp_min)\n",
    "    \n",
    "#     # find limits\n",
    "#     upper = df.temp_min.mean() + 3*df.temp_min.std()\n",
    "#     lower = df.temp_min.mean() - 3*df.temp_min.std()\n",
    "#     #print('upper limit:', upper)\n",
    "#     #print('lower limit:', lower)\n",
    "    \n",
    "#     # find outliers \n",
    "#     df.loc[(df.temp_min > upper) | (df.temp_min < lower)]\n",
    "#     print(len(df.loc[(df.temp_min > upper) | (df.temp_min < lower)]))\n",
    "    \n",
    "#     # trimming - replace the outlier data with NaN values\n",
    "#     for i in range(len(df)-1):\n",
    "#         if (df.loc[i, 'temp_min'] > upper) | (df.loc[i, 'temp_min'] < lower):\n",
    "#             df.loc[i, 'temp_min'] = float('nan')\n",
    "    \n",
    "#     #sns.displot(df.temp_min)\n",
    "#     print('after:')\n",
    "#     print(df.describe())\n",
    "\n",
    "#     # To save to the 8_temps_outliers folder\n",
    "#     path = \"D:/Allaus/Data_analysis/Neu_from_meteo/8_temps_outliers\"\n",
    "#     df.to_csv(Path(path, name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4216908d",
   "metadata": {},
   "source": [
    "## Block 7\n",
    "To compute snow accumulation of 48hr and 72hr, and average temperature.\n",
    "\n",
    "Also, to compute extreme rainfall using the POT (peak over threshold technique for q=95%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd85147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/8_temps_outliers\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # To grab just the name of each file\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "#     #print(name)\n",
    "    \n",
    "#         # To read the data\n",
    "#     if name != 'Ransol':\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')   \n",
    "#     else:\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "    \n",
    "#     if name != 'Ransol':\n",
    "#         df = df[~df.date.dt.hour.isin([13,13])]    #### Important!\n",
    "        \n",
    "#     if name != 'Ransol':\n",
    "#         df['snow_accum_72h'] = df.rolling(window = 3).snow_new.sum()\n",
    "#         df['snow_accum_48h'] = df.rolling(window = 2).snow_new.sum()\n",
    "#         df['temp_med'] = (df['temp_max'] + df['temp_min'])/2\n",
    "#     else:\n",
    "#         df['temp_med'] = (df['temp_max'] + df['temp_min'])/2\n",
    "\n",
    "#     #print(df)\n",
    "#     #df.snow_accum_72h.hist(figsize=(10,6), legend=True)\n",
    "    \n",
    "#     rain = df[['date', 'rain']]\n",
    "#     threshold = rain.rain.quantile(q=0.95)\n",
    "#     rain_indi = rain.index.to_list()\n",
    "    \n",
    "#     extremes = []\n",
    "#     indices = []\n",
    "#     for i in rain_indi:\n",
    "#         if rain.loc[i, 'rain'] > threshold:\n",
    "#             extremes.append(df.loc[i, 'rain'])\n",
    "#             indices.append(i)\n",
    "            \n",
    "#     extremes_df = pd.DataFrame(extremes, columns = ['extreme_rain'])\n",
    "#     indices_df = pd.DataFrame(indices, columns = ['index'])\n",
    "    \n",
    "#     xtrm = pd.concat([indices_df, extremes_df], axis=1)\n",
    "    \n",
    "#     spots = xtrm['index'].to_list()\n",
    "\n",
    "#     for i in spots:\n",
    "#         df.loc[i, 'extreme_rain'] = df.loc[i, 'rain']\n",
    "    \n",
    "#     #print(df)\n",
    "    \n",
    "#     # To save to the 9_snow_accum_temp_med_extreme_rain ONLY snow, temp and rain here\n",
    "#     path = \"D:/Allaus/Data_analysis/Neu_from_meteo/9_snow_accum_temp_med_extreme_rain ONLY snow, temp and rain here\"\n",
    "#     df.to_csv(Path(path, name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5676b5d",
   "metadata": {},
   "source": [
    "## Block 8\n",
    "To derive daily temperature range, temperature trends and moving average (2d + 3d) of rainfall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6665cf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/9_snow_accum_temp_med_extreme_rain ONLY snow, temp and rain here\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # To grab just the name of each file\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "#     print(name)\n",
    "    \n",
    "#     # To read the data\n",
    "#     df = pd.read_csv(f)\n",
    "#     df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "#     #print(df)\n",
    "    \n",
    "#     ### Compute moving average of rainfall (2d + 3d) ###\n",
    "#     df['rain3d'] = df.rolling(window = 3).rain.sum()\n",
    "#     df['rain2d'] = df.rolling(window = 2).rain.sum()\n",
    "    \n",
    "#     ### Compute daily temperature range ###\n",
    "    \n",
    "#     df['temp_range'] = df['temp_max'] - df['temp_min']\n",
    "#     #print(df)\n",
    "    \n",
    "#     ### Compute day-to-day temperature trend ###\n",
    "#     for i in range(0, len(df)-1):\n",
    "#         df.loc[i+1, 'temp_trend2d'] = df.loc[i+1, 'temp_med'] - df.loc[i, 'temp_med']\n",
    "#         df.loc[i+1, 'temp_trend2dabs'] = abs(df.loc[i+1, 'temp_med'] - df.loc[i, 'temp_med'])\n",
    "   \n",
    "#     for i in range(0, len(df)-2):\n",
    "#         df.loc[i+1, 'temp_trend3d'] = df.loc[i+2, 'temp_med'] - df.loc[i, 'temp_med']\n",
    "#         df.loc[i+1, 'temp_trend3dabs'] = abs(df.loc[i+1, 'temp_med'] - df.loc[i, 'temp_med']) + abs(df.loc[i+2, 'temp_med'] - df.loc[i+1, 'temp_med'])\n",
    "#     print(df)\n",
    "    \n",
    "#     # To save to the 15_temp_range_temp_trend\n",
    "#     path = \"D:/Allaus/Data_analysis/Neu_from_meteo/15_temp_range_temp_trend\"\n",
    "#     df.to_csv(Path(path, name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85125e1a",
   "metadata": {},
   "source": [
    "## Block 9\n",
    "To compute monthly averages of temp_max, temp_med, temp_min and rain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05112b8f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/9_snow_accum_temp_med_extreme_rain ONLY snow, temp and rain here\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "# stations = list()\n",
    "\n",
    "# Tmax = pd.DataFrame()\n",
    "# Tmin = pd.DataFrame()\n",
    "# Tmed = pd.DataFrame()\n",
    "# Rain = pd.DataFrame()\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # To grab just the name of each file\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "#     #print(name)\n",
    "    \n",
    "#     # To read the data\n",
    "#     if name != 'Ransol':\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "#     else:\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "\n",
    "#     df['day'] = df.date.dt.day\n",
    "#     df['month'] = df.date.dt.month\n",
    "#     df['year'] = df.date.dt.year\n",
    "#     df['hour'] = df.date.dt.hour\n",
    "#     #print(df)\n",
    "    \n",
    "#     df_temp_max = pd.DataFrame()\n",
    "#     df_temp_min = pd.DataFrame()\n",
    "#     df_temp_med = pd.DataFrame()\n",
    "#     df_rain = pd.DataFrame()\n",
    "\n",
    "\n",
    "#     df_temp_max = pd.DataFrame(df.groupby([df.year, df.month])['temp_max'].mean()).reset_index()\n",
    "#     df_temp_min = pd.DataFrame(df.groupby([df.year, df.month])['temp_min'].mean()).reset_index()\n",
    "#     df_temp_med = pd.DataFrame(df.groupby([df.year, df.month])['temp_med'].mean()).reset_index()\n",
    "#     df_rain = pd.DataFrame(df.groupby([df.year, df.month]).agg({'rain': sum})).reset_index()\n",
    "\n",
    "    \n",
    "#     stations.append(name)\n",
    "    \n",
    "#     temp_max = list()\n",
    "#     temp_min = list()\n",
    "#     temp_med = list()\n",
    "#     rain = list()\n",
    "\n",
    "    \n",
    "#     for i in [1,2,3,4,12]:\n",
    "#         temp_max.append(df_temp_max[df_temp_max.month == i]['temp_max'].mean())\n",
    "#         temp_min.append(df_temp_min[df_temp_min.month == i]['temp_min'].mean())\n",
    "#         temp_med.append(df_temp_med[df_temp_med.month == i]['temp_med'].mean())\n",
    "#         rain.append(df_rain[df_rain.month == i]['rain'].mean())\n",
    "      \n",
    "\n",
    "#     Tmax = Tmax.append(pd.DataFrame([temp_max], columns=['Jan', 'Feb', 'Mar', 'Apr', 'Dec']), ignore_index=True)\n",
    "#     Tmin = Tmin.append(pd.DataFrame([temp_min], columns=['Jan', 'Feb', 'Mar', 'Apr', 'Dec']), ignore_index=True)\n",
    "#     Tmed = Tmed.append(pd.DataFrame([temp_med], columns=['Jan', 'Feb', 'Mar', 'Apr', 'Dec']), ignore_index=True)\n",
    "#     Rain = Rain.append(pd.DataFrame([rain], columns=['Jan', 'Feb', 'Mar', 'Apr', 'Dec']), ignore_index=True)\n",
    "\n",
    "\n",
    "# #new_cols = ['Dec', 'Jan', 'Feb', 'Mar', 'Apr']\n",
    "\n",
    "# Tmax['station'] = stations\n",
    "# Tmax.set_index('station', inplace=True)\n",
    "# # Tmax.drop(columns=['May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov'], inplace=True)\n",
    "# # Tmax = Tmax.reindex(columns=new_cols)\n",
    "\n",
    "# Tmin['station'] = stations\n",
    "# Tmin.set_index('station', inplace=True)\n",
    "\n",
    "# Tmed['station'] = stations\n",
    "# Tmed.set_index('station', inplace=True)\n",
    "\n",
    "# Rain['station'] = stations\n",
    "# Rain.set_index('station', inplace=True)\n",
    "\n",
    "\n",
    "# print('historical monthly average maximum temperature:')\n",
    "# print(Tmax)\n",
    "# print()\n",
    "\n",
    "# print('historical monthly average minimum temperature:')\n",
    "# print(Tmin)\n",
    "# print()\n",
    "\n",
    "# print('historical monthly average medium temperature:')\n",
    "# print(Tmed)\n",
    "# print()\n",
    "\n",
    "# print('historical monthly average rainfall:')\n",
    "# print(Rain)\n",
    "# print()\n",
    "\n",
    "\n",
    "# # To save to the 10_monthly_averages folder\n",
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/10_monthly_averages ONLY snow, temp and rain here\"\n",
    "# Tmax.to_csv(Path(path, 'Tmax.csv'), index=True)\n",
    "# Tmin.to_csv(Path(path, 'Tmin.csv'), index=True)\n",
    "# Tmed.to_csv(Path(path, 'Tmed.csv'), index=True)\n",
    "# Rain.to_csv(Path(path, 'Rain.csv'), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d164d3",
   "metadata": {},
   "source": [
    "## Block 10\n",
    "Same as Block 9 but for snow_tot, snow_accum_72, snow_accum_48 and snow_new (the last one is basically snow_accum_24)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168b71e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/9_snow_accum_temp_med_extreme_rain ONLY snow, temp and rain here\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "# stations = list()\n",
    "\n",
    "# SnowTot = pd.DataFrame()\n",
    "# SnowNew = pd.DataFrame()\n",
    "# Snow48 = pd.DataFrame()\n",
    "# Snow72 = pd.DataFrame()\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # To grab just the name of each file\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "#     #print(name)\n",
    "    \n",
    "#     # To read the data\n",
    "#     if name != 'Ransol':\n",
    "#         stations.append(name) \n",
    "\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "#         #df.snow_accum_72h.hist()\n",
    "\n",
    "#         df['day'] = df.date.dt.day\n",
    "#         df['month'] = df.date.dt.month\n",
    "#         df['year'] = df.date.dt.year\n",
    "#         df['hour'] = df.date.dt.hour\n",
    "#         #print(df)\n",
    "    \n",
    "#         df_snow_tot = pd.DataFrame()\n",
    "#         df_snow_new = pd.DataFrame()\n",
    "#         df_snow_48 = pd.DataFrame()\n",
    "#         df_snow_72 = pd.DataFrame()\n",
    "    \n",
    "#         df_snow_tot = pd.DataFrame(df.groupby([df.year, df.month])['snow_tot'].mean()).reset_index()\n",
    "#         df_snow_new = pd.DataFrame(df.groupby([df.year, df.month])['snow_new'].max()).reset_index()\n",
    "#         df_snow_48 = pd.DataFrame(df.groupby([df.year, df.month])['snow_accum_48h'].max()).reset_index()\n",
    "#         df_snow_72 = pd.DataFrame(df.groupby([df.year, df.month])['snow_accum_72h'].max()).reset_index()\n",
    "        \n",
    "#         #print(df_snow_48)\n",
    "#         #print(df_snow_72)\n",
    "    \n",
    "#         snowTot = list()\n",
    "#         snowNew = list()\n",
    "#         snow48 = list()\n",
    "#         snow72 = list()\n",
    "    \n",
    "#         for i in [1,2,3,4,12]:\n",
    "#             snowTot.append(df_snow_tot[df_snow_tot.month == i]['snow_tot'].mean())\n",
    "#             snowNew.append(df_snow_new[df_snow_new.month == i]['snow_new'].mean())\n",
    "#             snow48.append(df_snow_48[df_snow_48.month == i]['snow_accum_48h'].mean())\n",
    "#             snow72.append(df_snow_72[df_snow_72.month == i]['snow_accum_72h'].mean())\n",
    "\n",
    "#         #print(snow48)\n",
    "#         #print(snow72)\n",
    "\n",
    "#         SnowTot = SnowTot.append(pd.DataFrame([snowTot], columns=['Jan', 'Feb', 'Mar', 'Apr', 'Dec']), ignore_index=True)\n",
    "#         SnowNew = SnowNew.append(pd.DataFrame([snowNew], columns=['Jan', 'Feb', 'Mar', 'Apr', 'Dec']), ignore_index=True)\n",
    "#         Snow48 = Snow48.append(pd.DataFrame([snow48], columns=['Jan', 'Feb', 'Mar', 'Apr', 'Dec']), ignore_index=True)\n",
    "#         Snow72 = Snow72.append(pd.DataFrame([snow72], columns=['Jan', 'Feb', 'Mar', 'Apr', 'Dec']), ignore_index=True)\n",
    "\n",
    "# SnowTot['station'] = stations\n",
    "# SnowTot.set_index('station', inplace=True)\n",
    "\n",
    "# SnowNew['station'] = stations\n",
    "# SnowNew.set_index('station', inplace=True)\n",
    "    \n",
    "# Snow48['station'] = stations\n",
    "# Snow48.set_index('station', inplace=True)\n",
    "\n",
    "# Snow72['station'] = stations\n",
    "# Snow72.set_index('station', inplace=True)\n",
    "\n",
    "# print('historical monthly average snow thickness of 48 hours:')\n",
    "# print(SnowTot)\n",
    "# print()\n",
    "\n",
    "# print('historical monthly maximum snow accummulation/new snow of 24 hours:')\n",
    "# print(SnowNew)\n",
    "# print()\n",
    "\n",
    "# print('historical monthly maximum snow accummulation/new snow of 48 hours:')\n",
    "# print(Snow48)\n",
    "# print()\n",
    "\n",
    "# print('historical monthly maximum snow accummulation/new snow of 72 hours:')\n",
    "# print(Snow72)\n",
    "\n",
    "# # To save to the 10_monthly_averages folder\n",
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/10_monthly_averages ONLY, snow temp and rain here\"\n",
    "# SnowTot.to_csv(Path(path, 'SnowTot.csv'), index=True)\n",
    "# SnowNew.to_csv(Path(path, 'SnowNew.csv'), index=True)\n",
    "# Snow48.to_csv(Path(path, 'Snow48.csv'), index=True)\n",
    "# Snow72.to_csv(Path(path, 'Snow72.csv'), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e14a0",
   "metadata": {},
   "source": [
    "## Block 11\n",
    "Calculate yearly precipitation (total) for Double-mass curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f08571",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/9_snow_accum_temp_med_extreme_rain ONLY snow, temp and rain here\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # To grab just the name of each file\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "#     print(name)\n",
    "    \n",
    "#     # To read the data\n",
    "#     if name != 'Ransol':\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "#     else:\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "\n",
    "#     df['day'] = df.date.dt.day\n",
    "#     df['month'] = df.date.dt.month\n",
    "#     df['year'] = df.date.dt.year\n",
    "#     df['hour'] = df.date.dt.hour\n",
    "#     #print(df)\n",
    "    \n",
    "#     df_rain = pd.DataFrame()\n",
    "\n",
    "#     df_rain = pd.DataFrame(df.groupby(df.year).agg({'rain': sum})).reset_index()\n",
    "#     print(df_rain)\n",
    "\n",
    "\n",
    "#     # To save to the 11_total_annual_rain\n",
    "#     path = \"D:/Allaus/Data_analysis/Neu_from_meteo/11_total_annual_rain\"\n",
    "#     df_rain.to_csv(Path(path, name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3738d4",
   "metadata": {},
   "source": [
    "## Block 12\n",
    "Calculate yearly snow_new values of stations (to check with Double-Mass Curve, although it is not accurate for snow I think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35052a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/8_temps_outliers\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # To grab just the name of each file\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "#     #print(name)\n",
    "    \n",
    "#     # To read the data\n",
    "#     if name == 'Ransol':\n",
    "#         pass\n",
    "#     else:\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "\n",
    "#         df['day'] = df.date.dt.day\n",
    "#         df['month'] = df.date.dt.month\n",
    "#         df['year'] = df.date.dt.year\n",
    "#         df['hour'] = df.date.dt.hour\n",
    "#         #print(df)\n",
    "    \n",
    "#         df_snow = pd.DataFrame()\n",
    "\n",
    "#         df_snow = pd.DataFrame(df.groupby(df.year).agg({'snow_new': sum})).reset_index()\n",
    "#         #print(df_snow)\n",
    "\n",
    "\n",
    "#         # To save to the 12_check_snow_new folder\n",
    "#         path = \"D:/Allaus/Data_analysis/Neu_from_meteo/12_check_snow_new\"\n",
    "#         df_snow.to_csv(Path(path, name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a8d90",
   "metadata": {},
   "source": [
    "## Block 13\n",
    "Calculate yearly snow_tot values of stations (to check with Double-Mass Curve, although it is not accurate for snow I think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9b9626",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/8_temps_outliers\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # To grab just the name of each file\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "#     print(name)\n",
    "    \n",
    "#     # To read the data\n",
    "#     df = pd.read_csv(f)\n",
    "#     if name == 'Ransol':\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "#     else:\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "#     df['year'] = df.date.dt.year\n",
    "#     #print(df)\n",
    "\n",
    "#     df_snow = pd.DataFrame()\n",
    "\n",
    "#     df_snow = pd.DataFrame(df.groupby(df.year).agg({'snow_tot': sum})).reset_index()\n",
    "#     #print(df_snow)\n",
    "\n",
    "\n",
    "#     # To save to the 13_check_snow_tot folder\n",
    "#     path = \"D:/Allaus/Data_analysis/Neu_from_meteo/13_check_snow_tot\"\n",
    "#     df_snow.to_csv(Path(path, name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e4754",
   "metadata": {},
   "source": [
    "## Block 14\n",
    "To map degrees for snowdrift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c413610",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/7_map\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # To grab just the name of each file\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "#     #print(name)     # for debugging\n",
    "    \n",
    "#     # To read the data\n",
    "#     if name == 'Ransol':\n",
    "#         pass\n",
    "#     else:\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y %H:%M')\n",
    "\n",
    "#     #print(df)\n",
    "\n",
    "#         for i in range(len(df)-1):\n",
    "#             if df.loc[i, 'snowdrift'] == 2:\n",
    "#                 df.loc[i, 'snowdrift_from_deg'] = random.randint(68,112)\n",
    "#             elif df.loc[i, 'snowdrift'] == 3:\n",
    "#                 df.loc[i, 'snowdrift_from_deg'] = random.randint(158,202)\n",
    "#             elif df.loc[i, 'snowdrift'] == 4:\n",
    "#                 df.loc[i, 'snowdrift_from_deg'] = random.randint(248,292)\n",
    "#             elif df.loc[i, 'snowdrift'] == 5:\n",
    "#                 df.loc[i, 'snowdrift_from_deg'] = random.randint(*random.choice([(338, 360), (0, 22)]))\n",
    "#             elif df.loc[i, 'snowdrift'] == 6:\n",
    "#                 df.loc[i, 'snowdrift_from_deg'] = random.randint(68,112)\n",
    "#             elif df.loc[i, 'snowdrift'] == 7:\n",
    "#                 df.loc[i, 'snowdrift_from_deg'] = random.randint(158,202)\n",
    "#             elif df.loc[i, 'snowdrift'] == 8:\n",
    "#                 df.loc[i, 'snowdrift_from_deg'] = random.randint(248,292)\n",
    "#             elif df.loc[i, 'snowdrift'] == 9:\n",
    "#                 df.loc[i, 'snowdrift_from_deg'] = random.randint(*random.choice([(338, 360), (0, 22)]))\n",
    "#             else:\n",
    "#                 df.loc[i, 'snowdrift_from_deg'] = float('nan')\n",
    "  \n",
    "#     #print(df)  #to check\n",
    "        \n",
    "#     # To save to the 14_map_snowdrift_from_deg folder\n",
    "#     path = \"D:/Allaus/Data_analysis/Neu_from_meteo/14_map_snowdrift_from_deg\"\n",
    "#     df.to_csv(Path(path, name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976273c7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/9_snow_accum_temp_med_extreme_rain ONLY snow, temp and rain here\"\n",
    "# csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# #print(csv_files)\n",
    "\n",
    "# stations = list()\n",
    "\n",
    "# SnowNew = pd.DataFrame()\n",
    "\n",
    "# for f in csv_files:\n",
    "#     # To grab just the name of each file\n",
    "#     file_name = os.path.basename(f)\n",
    "#     file = os.path.splitext(file_name)\n",
    "#     name = file[0]\n",
    "#     #print(name)\n",
    "    \n",
    "#     # To read the data\n",
    "#     if name != 'Ransol':\n",
    "#         stations.append(name) \n",
    "\n",
    "#         df = pd.read_csv(f)\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#         df['day'] = df.date.dt.day\n",
    "#         df['month'] = df.date.dt.month\n",
    "#         df['year'] = df.date.dt.year\n",
    "#         df['hour'] = df.date.dt.hour\n",
    "#         #print(df)\n",
    "    \n",
    "#         df_snow_new = pd.DataFrame() \n",
    "#         df_snow_new = pd.DataFrame(df.groupby([df.year, df.month]).agg({'snow_new': sum})).reset_index()\n",
    "        \n",
    "#         #print(df_snow_new)\n",
    "    \n",
    "#         snowNew = list()\n",
    "\n",
    "#         for i in [1,2,3,4,12]:\n",
    "#             snowNew.append(df_snow_new[df_snow_new.month == i]['snow_new'].mean())\n",
    "\n",
    "#         #print(snowNew)\n",
    "\n",
    "#         SnowNew = SnowNew.append(pd.DataFrame([snowNew], columns=['Jan', 'Feb', 'Mar', 'Apr', 'Dec']), ignore_index=True)\n",
    "\n",
    "# SnowNew['station'] = stations\n",
    "# SnowNew.set_index('station', inplace=True)\n",
    "\n",
    "# print('historical monthly maximum snow accummulation/new snow of 24 hours:')\n",
    "# print(SnowNew)\n",
    "# print()\n",
    "\n",
    "# # To save to the 16_monthly_snow_accum folder\n",
    "# path = \"D:/Allaus/Data_analysis/Neu_from_meteo/16_monthly_snow_accum\"\n",
    "# SnowNew.to_csv(Path(path, 'monthly_snow_accum.csv'), index=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
